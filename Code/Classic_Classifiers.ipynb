{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9KSFEL5X61p",
        "outputId": "ed18fe5e-8fe3-4c12-a185-9b4d2f88fced"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tweet-preprocessor\n",
            "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting greek_stemmer\n",
            "  Downloading greek_stemmer-0.1.1.tar.gz (6.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from greek_stemmer) (6.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from greek_stemmer) (0.18.3)\n",
            "Building wheels for collected packages: greek_stemmer\n",
            "  Building wheel for greek_stemmer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for greek_stemmer: filename=greek_stemmer-0.1.1-py3-none-any.whl size=6721 sha256=1af49deddb51f2b0c036269c95580f6199e2e04f1782d18fda966dfd5acad95a\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/5f/74/41c1d13e787f8aa958796c4fdc1738bb11afac2df1d4c6d815\n",
            "Successfully built greek_stemmer\n",
            "Installing collected packages: greek_stemmer\n",
            "Successfully installed greek_stemmer-0.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyyaml==5.4.1\n",
            "  Downloading PyYAML-5.4.1.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.1/175.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pyyaml\n",
            "  Building wheel for pyyaml (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.4.1-cp310-cp310-linux_x86_64.whl size=45658 sha256=0ed866393ad6559fc61cb920332e63fb930a9a52f98a13f5e90845bd5332ab05\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/0d/22/696ee92245ad710f506eee79bb05c740d8abccd3ecdb778683\n",
            "Successfully built pyyaml\n",
            "Installing collected packages: pyyaml\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "Successfully installed pyyaml-5.4.1\n"
          ]
        }
      ],
      "source": [
        "# Install libraries\n",
        "!pip install tweet-preprocessor\n",
        "!pip install greek_stemmer\n",
        "!pip install pyyaml==5.4.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load general libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "\n",
        "# Load preprocessing libraries\n",
        "import pandas as pd\n",
        "import preprocessor as p\n",
        "import unicodedata as ud\n",
        "from sklearn.model_selection import train_test_split, PredefinedSplit\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from greek_stemmer import GreekStemmer\n",
        "\n",
        "# Load classifier libraries\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Load evaluation libraries\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, precision_recall_fscore_support, classification_report\n",
        "\n",
        "# Load imbalance libraries\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# saving/loading sklearn models\n",
        "from joblib import dump, load"
      ],
      "metadata": {
        "id": "w_97xXjNYAFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b635ccc1-cf1b-4ff8-fb73-89d52c85e282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess tweets\n",
        "def preprocess_tweets(X):\n",
        "    X = clean_text(X)\n",
        "    X = use_stemming(X)\n",
        "    return X\n",
        "\n",
        "def clean_text(X):\n",
        "    X = [tweet.lower() for tweet in X]\n",
        "\n",
        "    # Remove URLS, mentions\n",
        "    p.set_options(p.OPT.URL, p.OPT.MENTION)\n",
        "    X = [p.clean(tweet) for tweet in X]\n",
        "\n",
        "    # Remove Greek accents\n",
        "    d = {ord('\\N{COMBINING ACUTE ACCENT}'):None}\n",
        "    X = [ud.normalize('NFD',word).translate(d) for word in X]\n",
        "    return X\n",
        "\n",
        "def use_stemming(X):\n",
        "    stemmer = GreekStemmer()\n",
        "    X_new = []\n",
        "    for tweet in X:\n",
        "        tokens = word_tokenize(tweet, language=\"greek\")\n",
        "        X_new.append(\" \".join([stemmer.stem(word.upper()) for word in tokens]))\n",
        "    return X_new\n",
        "\n",
        "def resampling(X_train, y_train, strategy='auto'):\n",
        "  #Oversample minority classes(positive, negative) to numbers of majority class(neutral)\n",
        "  if strategy == 'auto':\n",
        "    ros = RandomOverSampler(sampling_strategy=strategy, random_state=42)\n",
        "    X_res, y_res = ros.fit_resample(X_train, y_train)\n",
        "  #Oversample minority class(positive) to numbers of majority class(neutral), then undersample positive and neutral to numbers of negative class\n",
        "  elif strategy == 'combination':\n",
        "    ros = RandomOverSampler(sampling_strategy='minority', random_state=42)\n",
        "    X_ros, y_ros = ros.fit_resample(X_train, y_train)\n",
        "    rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
        "    X_res, y_res = rus.fit_resample(X_ros, y_ros)\n",
        "\n",
        "  return X_res, y_res\n"
      ],
      "metadata": {
        "id": "U8yYS2glYB4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a standard (sklearn) classifier using grid search with selected validation split\n",
        "def train_standard_classifier(classifier, X_model, y_model, X_test, y_test, val_split, weight):\n",
        "    # Set hyperparameters to optimize in each case\n",
        "    if classifier == \"RF\":\n",
        "      clf = RandomForestClassifier(random_state=42, class_weight=weight)\n",
        "      param_grid = { 'max_depth': [10, 50, 100, None],\n",
        "                    'max_features': ['sqrt'],\n",
        "                    'min_samples_leaf': [1, 2, 4],\n",
        "                    'min_samples_split': [2, 5, 10],\n",
        "                    'n_estimators': [10, 100, 200]}\n",
        "    elif classifier == \"DT\":\n",
        "      clf = DecisionTreeClassifier(random_state=42, class_weight=weight)\n",
        "      param_grid = {'criterion' : [\"gini\", \"entropy\", \"log_loss\"],\n",
        "                    'max_depth': [10, 40, 70, 100, 130, 160, 190, 220, 250, None],\n",
        "                    'max_features': ['sqrt'],\n",
        "                    'min_samples_leaf': [1, 2, 4],\n",
        "                    'min_samples_split': [2, 5, 10]}\n",
        "    elif classifier == \"GB\":\n",
        "       clf = GradientBoostingClassifier(random_state=42)\n",
        "       param_grid = {'max_depth': [10, 50, 100, None],\n",
        "                    'max_features': ['sqrt'],\n",
        "                    'min_samples_leaf': [1, 2, 5],\n",
        "                    'min_samples_split': [2, 10]}\n",
        "    elif classifier == \"NB\":\n",
        "      clf = GaussianNB()\n",
        "      param_grid = {'var_smoothing' : [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}\n",
        "    elif classifier == \"SVM\":\n",
        "       clf = SVC(random_state=42, class_weight=weight)\n",
        "       param_grid = {'C': [0.1, 1, 10, 100],\n",
        "              'gamma': [1, 0.1, 0.01],\n",
        "              'kernel': [\"poly\", \"rbf\"],\n",
        "              'decision_function_shape': [\"ovr\"]}\n",
        "    else:\n",
        "        return None\n",
        "    print(\"fitting model\", classifier)\n",
        "    vps = PredefinedSplit(test_fold=val_split)\n",
        "    CV = GridSearchCV(estimator=clf, scoring=\"f1_macro\", param_grid=param_grid, cv=vps)\n",
        "    start = time()\n",
        "    CV.fit(X_model, y_model)\n",
        "    train_time = time() - start\n",
        "    print(\"Fitting ended, time required:\", train_time, \"seconds.\")\n",
        "    print(\"Best method params:\", CV.best_params_)\n",
        "    y_pred = CV.predict(X_test)\n",
        "    multi_metrics = precision_recall_fscore_support(y_test, y_pred)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(\"Precision =\", multi_metrics[0])\n",
        "    print(\"Recall =\", multi_metrics[1])\n",
        "    print(\"f1 =\", multi_metrics[2])\n",
        "    print(\"Accuracy =\", acc)\n",
        "    print()\n",
        "    return CV.best_estimator_, train_time, [multi_metrics[0], multi_metrics[1], multi_metrics[2],\n",
        "                                            [precision_score(y_test, y_pred, average='macro'), recall_score(y_test, y_pred, average='macro'),\n",
        "                                             f1_score(y_test, y_pred, average='macro'), acc]]\n",
        "\n",
        "# Selecting Classifier type among all major categories\n",
        "def select_classifier(classifier, X_model, y_model, X_test, y_test, split_index, weight):\n",
        "    if classifier == \"LSTM\":\n",
        "        return train_LSTM()\n",
        "    elif classifier == \"BERT\":\n",
        "        return train_BERT()\n",
        "    else:\n",
        "        return train_standard_classifier(classifier, X_model, y_model, X_test, y_test, split_index, weight)"
      ],
      "metadata": {
        "id": "evuOmUKvYjSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve 2015 dataset from Github (!!!!!should probably change link at some point)\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/dimosbele/sentiment_analysis_greek/master/modeling/data/df_final.pkl\"\n",
        "dataset = pd.read_pickle(url)\n",
        "y = np.array(dataset[\"Sentiment\"])\n",
        "for i in range(len(y)):\n",
        "    y[i] += 1"
      ],
      "metadata": {
        "id": "vW-60St4YSAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorizing data for standard classifier\n",
        "\n",
        "def get_split_indexer(X_model, X_train):\n",
        "    return [(-1 if column in X_train.index else 0) for column in X_model.index]\n",
        "\n",
        "def vectorize(data, vectorizer):\n",
        "    X_vector = vectorizer.transform(data)\n",
        "    words = vectorizer.get_feature_names_out()\n",
        "    X_vector = pd.DataFrame(X_vector.toarray(), index=data.index)\n",
        "    X_vector.columns = words\n",
        "    return(X_vector)\n",
        "\n",
        "def create_sets(dataset, has_text=True, has_features=True, resampling_method=None):\n",
        "  data_X = dataset.drop([\"Text_only\", \"Tokens_stem\", \"Tokens\", \"Sentiment\", \"Bigrams2\", \"Match_Terms\"], axis=1)\n",
        "  data_X[\"Text\"] = preprocess_tweets(data_X[\"Text\"])\n",
        "  # splitting data in Model (Train and Validation) including split, as well as Test\n",
        "  X_model, X_test, y_model, y_test = train_test_split(data_X, y, test_size = 0.2, random_state=42, stratify=y)\n",
        "  X_train, X_val , y_train, y_val = train_test_split(X_model, y_model,test_size = 0.125, random_state=42, stratify=y_model)\n",
        "\n",
        "  # resample X_train/y_train, then merge with val data into new X_model and y_model\n",
        "\n",
        "  # splitting X data to vectorizables and non-vectorizables\n",
        "  X_model_text = X_model[\"Text\"]\n",
        "  X_test_text = X_test[\"Text\"]\n",
        "  X_train_text = train_text_domain = X_train[\"Text\"]\n",
        "  X_val_text = X_val[\"Text\"]\n",
        "\n",
        "  X_train_metadata = X_train.drop(columns=['Text'])\n",
        "  X_val_metadata = X_val.drop(columns=['Text'])\n",
        "  X_model_metadata = X_model.drop(columns=['Text'])\n",
        "  X_test_metadata = X_test.drop(columns=['Text'])\n",
        "\n",
        "  # vectorizing model from text dataframes\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  vectorizer_fit = vectorizer.fit(X_train_text)\n",
        "  X_model_text = vectorize(X_model_text, vectorizer_fit)\n",
        "  X_train_text = vectorize(X_train_text, vectorizer_fit)\n",
        "  X_val_text = vectorize(X_val_text, vectorizer_fit)\n",
        "  X_test_text = vectorize(X_test_text, vectorizer_fit)\n",
        "\n",
        "\n",
        "  if(has_text and has_features):\n",
        "    # merging subdataframes\n",
        "    X_train = X_train_text.join(X_train_metadata)\n",
        "    X_val = X_val_text.join(X_val_metadata)\n",
        "    X_test = X_test_text.join(X_test_metadata)\n",
        "    if(resampling_method is not None):\n",
        "      X_res, y_res = resampling(X_train, y_train, resampling_method)\n",
        "      #X_res = pd.DataFrame(X_res.toarray(), index=X_train.index)\n",
        "      y_res = pd.DataFrame(y_res)\n",
        "      y_val = pd.DataFrame(y_val)\n",
        "      X_model = pd.concat([X_val, X_res])\n",
        "      y_model = pd.concat([y_val, y_res]).to_numpy().ravel()\n",
        "    else:\n",
        "      X_model = X_model_text.join(X_model_metadata)\n",
        "    return X_model, y_model, X_test, y_test, X_train, y_train, train_text_domain\n",
        "  elif(has_text):\n",
        "    return X_model_text, y_model, X_test_text, y_test, X_train_text, y_train, train_text_domain\n",
        "  elif(has_features):\n",
        "    return X_model_metadata, y_model, X_test_metadata, y_test, X_train_metadata, y_train, train_text_domain\n",
        "  else:\n",
        "    print(\"error on inputs\")\n",
        "    return None, None, None, None, None, None\n",
        "\n",
        "def save_models(best_estimators, has_text=False, has_imb=\"\"):\n",
        "  IMBL_String = \"\"\n",
        "  Text_String = \"_Features\"\n",
        "  if(has_imb == 'auto'):\n",
        "    IMBL_String = \"_IMB_auto\"\n",
        "  elif(has_imb == 'combination'):\n",
        "    IMBL_String = \"_IMB_combination\"\n",
        "  if(has_text):\n",
        "    Text_String = \"_Features_and_Text\"\n",
        "  models = [\"DT\", \"GB\", \"RF\", \"NB\", \"SVM\"]\n",
        "  for estimator, model in zip(best_estimators, models):\n",
        "    String = \"best_\" + model + Text_String + IMBL_String + \".joblib\"\n",
        "    dump(estimator, String)\n",
        "\n",
        "def load_models(has_text=False, has_imb=\"\"):\n",
        "  IMBL_String = \"\"\n",
        "  Text_String = \"_Features\"\n",
        "  if(has_imb == 'auto'):\n",
        "    IMBL_String = \"_IMB_auto\"\n",
        "  elif(has_imb == 'combination'):\n",
        "    IMBL_String = \"_IMB_combination\"\n",
        "  if(has_text):\n",
        "    Text_String = \"_Features_and_Text\"\n",
        "  models = [\"DT\", \"GB\", \"RF\", \"NB\", \"SVM\"]\n",
        "  best_estimators = []\n",
        "  for model in models:\n",
        "    String = \"best_\" + model + Text_String + IMBL_String + \".joblib\"\n",
        "    estimator = load(String)\n",
        "    best_estimators.append(estimator)\n",
        "  return best_estimators\n"
      ],
      "metadata": {
        "id": "eRRVSFbxYoAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_model, y_model, X_test, y_test, X_train, y_train, train_text = create_sets(dataset, True, True, None)\n",
        "weight = \"balanced\"\n",
        "split_index = get_split_indexer(X_model, X_train)\n",
        "print(\"Train/Val shape:\", X_model.shape)\n",
        "print(\"Train shape:\", X_train.shape)\n",
        "print(\"Test Shape:\", X_test.shape)\n",
        "print(\"Split Index length:\", len(split_index))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRDG7dGLYx8C",
        "outputId": "10b68b03-3052-4fe8-e7e0-d6436bfdda38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/greek_stemmer/__init__.py:340: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
            "  custom_rules = yaml.load(f.read())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Val shape: (1312, 2733)\n",
            "Train shape: (1148, 2733)\n",
            "Test Shape: (328, 2733)\n",
            "Split Index length: 1312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_model, y_model, X_test, y_test, X_train, y_train, train_text = create_sets(dataset, True, False, \"auto\")\n",
        "weight = None\n",
        "split_index = get_split_indexer(X_model, X_train)\n",
        "print(\"Train/Val shape:\", X_model.shape, y_model.shape)\n",
        "print(\"Train shape:\", X_train.shape)\n",
        "print(\"Test Shape:\", X_test.shape)\n",
        "print(\"Split Index length:\", len(split_index))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBzcxbViO5kl",
        "outputId": "4593da80-2a10-408c-b8b5-67bc710fb823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/greek_stemmer/__init__.py:340: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
            "  custom_rules = yaml.load(f.read())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Val shape: (1312, 2704) (1312,)\n",
            "Train shape: (1148, 2704)\n",
            "Test Shape: (328, 2704)\n",
            "Split Index length: 1312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Classifiers\n",
        "best_val_models = []\n",
        "best_scores = []\n",
        "train_times = []\n",
        "models = [\"DT\", \"GB\", \"RF\", \"NB\", \"SVM\"]\n",
        "print(\"weight is \", weight)\n",
        "for model in models:\n",
        "  classifier = model\n",
        "  good_model, train_time, scores = select_classifier(classifier, X_model, y_model, X_test, y_test, split_index, weight)\n",
        "  best_val_models.append(good_model)\n",
        "  best_scores.append(scores)\n",
        "  train_times.append(train_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLdwqwFoYbQy",
        "outputId": "f5828ad3-ea6c-405d-f20c-639e0a3f9fdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weight is  None\n",
            "fitting model DT\n",
            "Fitting ended, time required: 12.088324785232544 seconds.\n",
            "Best method params: {'criterion': 'gini', 'max_depth': 70, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
            "Precision = [0.73267327 0.77619048 0.23529412]\n",
            "Recall = [0.63793103 0.83163265 0.25      ]\n",
            "f1 = [0.68202765 0.80295567 0.24242424]\n",
            "Accuracy = 0.7347560975609756\n",
            "\n",
            "fitting model GB\n",
            "Fitting ended, time required: 115.36433172225952 seconds.\n",
            "Best method params: {'max_depth': 100, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
            "Precision = [0.80392157 0.81363636 1.        ]\n",
            "Recall = [0.70689655 0.91326531 0.375     ]\n",
            "f1 = [0.75229358 0.86057692 0.54545455]\n",
            "Accuracy = 0.8140243902439024\n",
            "\n",
            "fitting model RF\n",
            "Fitting ended, time required: 59.56161308288574 seconds.\n",
            "Best method params: {'max_depth': 100, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 10}\n",
            "Precision = [0.8        0.81081081 0.83333333]\n",
            "Recall = [0.68965517 0.91836735 0.3125    ]\n",
            "f1 = [0.74074074 0.86124402 0.45454545]\n",
            "Accuracy = 0.8079268292682927\n",
            "\n",
            "fitting model NB\n",
            "Fitting ended, time required: 0.34198522567749023 seconds.\n",
            "Best method params: {'var_smoothing': 1e-09}\n",
            "Precision = [0.7295082  0.85638298 0.33333333]\n",
            "Recall = [0.76724138 0.82142857 0.375     ]\n",
            "f1 = [0.74789916 0.83854167 0.35294118]\n",
            "Accuracy = 0.7804878048780488\n",
            "\n",
            "fitting model SVM\n",
            "Fitting ended, time required: 59.95925045013428 seconds.\n",
            "Best method params: {'C': 100, 'decision_function_shape': 'ovr', 'gamma': 0.1, 'kernel': 'rbf'}\n",
            "Precision = [0.8125    0.8592233 0.5      ]\n",
            "Recall = [0.78448276 0.90306122 0.3125    ]\n",
            "f1 = [0.79824561 0.88059701 0.38461538]\n",
            "Accuracy = 0.8323170731707317\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_models(best_val_models, True, \"auto\")"
      ],
      "metadata": {
        "id": "EKuhA7nuZ65j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loaded_models = load_models()\n",
        "for model in best_val_models:\n",
        "  print(model)\n",
        "  y_pred = model.predict(X_test)\n",
        "  print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGgYM7t5dp3h",
        "outputId": "7877af77-2c9c-43a0-d7e0-96067799b526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecisionTreeClassifier(criterion='entropy', max_depth=40, max_features='sqrt',\n",
            "                       random_state=42)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.62      0.62       116\n",
            "           1       0.77      0.77      0.77       196\n",
            "           2       0.31      0.31      0.31        16\n",
            "\n",
            "    accuracy                           0.69       328\n",
            "   macro avg       0.57      0.57      0.57       328\n",
            "weighted avg       0.69      0.69      0.69       328\n",
            "\n",
            "GradientBoostingClassifier(max_depth=10, max_features='sqrt',\n",
            "                           min_samples_leaf=2, random_state=42)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.75      0.79       116\n",
            "           1       0.84      0.91      0.88       196\n",
            "           2       0.70      0.44      0.54        16\n",
            "\n",
            "    accuracy                           0.83       328\n",
            "   macro avg       0.79      0.70      0.73       328\n",
            "weighted avg       0.83      0.83      0.83       328\n",
            "\n",
            "RandomForestClassifier(max_depth=50, random_state=42)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.73      0.77       116\n",
            "           1       0.82      0.91      0.87       196\n",
            "           2       1.00      0.31      0.48        16\n",
            "\n",
            "    accuracy                           0.82       328\n",
            "   macro avg       0.88      0.65      0.70       328\n",
            "weighted avg       0.83      0.82      0.81       328\n",
            "\n",
            "GaussianNB(var_smoothing=1e-06)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.80      0.77       116\n",
            "           1       0.87      0.81      0.84       196\n",
            "           2       0.32      0.38      0.34        16\n",
            "\n",
            "    accuracy                           0.78       328\n",
            "   macro avg       0.64      0.66      0.65       328\n",
            "weighted avg       0.79      0.78      0.79       328\n",
            "\n",
            "SVC(C=0.1, gamma=0.1, kernel='poly', random_state=42)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.68      0.74       116\n",
            "           1       0.83      0.90      0.87       196\n",
            "           2       0.42      0.50      0.46        16\n",
            "\n",
            "    accuracy                           0.80       328\n",
            "   macro avg       0.69      0.69      0.69       328\n",
            "weighted avg       0.81      0.80      0.80       328\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_models = load_models(True, \"auto\")"
      ],
      "metadata": {
        "id": "jqw236dcRm9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GB_model = loaded_models[1]\n",
        "X_elec = pd.read_csv(\"/content/sample_data/Εκλογές_2023_final.csv\")\n",
        "X_mits = pd.read_csv(\"/content/sample_data/Μητσοτακης_final.csv\")\n",
        "X_nd = pd.read_csv(\"/content/sample_data/ΝΔ_final.csv\")\n",
        "X_sir = pd.read_csv(\"/content/sample_data/ΣΥΡΙΖΑ_final.csv\")\n",
        "X_tsip = pd.read_csv(\"/content/sample_data/Τσιπρας_final.csv\")\n",
        "\n",
        "X_elec = preprocess_tweets(X_elec[\"0\"])\n",
        "X_mits = preprocess_tweets(X_mits[\"0\"])\n",
        "X_nd = preprocess_tweets(X_nd[\"0\"])\n",
        "X_sir = preprocess_tweets(X_sir[\"0\"])\n",
        "X_tsip = preprocess_tweets(X_tsip[\"0\"])\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(train_text)\n",
        "X_elec = vectorizer.transform(X_elec)\n",
        "X_mits = vectorizer.transform(X_mits)\n",
        "X_nd = vectorizer.transform(X_nd)\n",
        "X_sir = vectorizer.transform(X_sir)\n",
        "X_tsip = vectorizer.transform(X_tsip)\n",
        "\n",
        "y_elec = GB_model.predict(X_elec)\n",
        "y_mits = GB_model.predict(X_mits)\n",
        "y_nd = GB_model.predict(X_nd)\n",
        "y_sir = GB_model.predict(X_sir)\n",
        "y_tsip = GB_model.predict(X_tsip)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjDzKv1BRm4d",
        "outputId": "cfcdb151-dc0e-4e7c-b1ea-6c00f9067908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/greek_stemmer/__init__.py:340: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
            "  custom_rules = yaml.load(f.read())\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(y_elec).to_csv(\"/content/sample_data/GB_elections.csv\")\n",
        "pd.DataFrame(y_mits).to_csv(\"/content/sample_data/GB_Mitsotakis.csv\")\n",
        "pd.DataFrame(y_nd).to_csv(\"/content/sample_data/GB_ND.csv\")\n",
        "pd.DataFrame(y_sir).to_csv(\"/content/sample_data/GB_Siriza.csv\")\n",
        "pd.DataFrame(y_tsip).to_csv(\"/content/sample_data/GB_Tsipras.csv\")"
      ],
      "metadata": {
        "id": "QGwzN-eHgP6m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}